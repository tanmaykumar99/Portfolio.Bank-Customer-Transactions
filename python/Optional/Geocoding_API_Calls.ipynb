{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392de586-cc65-441b-b888-ce29dc5d9413",
   "metadata": {},
   "source": [
    "For a large, messy dataframe such as in Bank Transactions.csv, one can also make use of OpenMaps or Google \n",
    "Geocoding API to clean the CustLocation column. Given below are both their executions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438f1ab-6792-4947-a9b3-ff182f64d34c",
   "metadata": {},
   "source": [
    "## Google Maps Geocoding\n",
    "(Most accurate, but requires an API key and billing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516fc03-e38c-409b-8e1a-dc8f5488edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "api_key = \"xyz_api_key\" ## Replace with the Google maps API key generated for you\n",
    "input_docx = \"/mnt/data/Todcheck.docx\" # or path to input CSV with one column 'location'\n",
    "cache_file = \"geocode_cache.json\"\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "rate_sleep = 0.05 # Should be adjusted accordinly if quota is hit\n",
    "\n",
    "def load_docx_lines(path):\n",
    "    from docx import Document\n",
    "    doc = Document(path)\n",
    "    lines = []\n",
    "    for p in doc.paragraphs:\n",
    "        txt = p.text.strip()\n",
    "        if txt:\n",
    "            # removing leading numbering like \"1. \"\n",
    "            txt = re.sub(r'^\\d+\\.\\s*', '', txt)\n",
    "            lines.append(txt)\n",
    "    return lines\n",
    "\n",
    "def normalize(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = re.sub(r'\\s+', ' ', str(s).strip())\n",
    "    return s\n",
    "\n",
    "def load_cache(fn):\n",
    "    if Path(fn).exists():\n",
    "        return json.loads(Path(fn).read_text(encoding=\"utf8\"))\n",
    "    return {}\n",
    "\n",
    "def save_cache(fn, data):\n",
    "    Path(fn).write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf8\")\n",
    "\n",
    "def google_geocode(address, api_key):\n",
    "    # call Places/Geocoding endpoint\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\"address\": address, \"key\": api_key, \"region\": \"IN\"}  # hint for region\n",
    "    resp = requests.get(url, params=params, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def get_country_from_result(geo_json):\n",
    "    \n",
    "    # extraction of country and ISO\n",
    "    if not geo_json:\n",
    "        return None, None\n",
    "    results = geo_json.get(\"results\", [])\n",
    "    if not results:\n",
    "        return None, None\n",
    "        \n",
    "    # using best (first) result\n",
    "    comp = results[0].get(\"address_components\", [])\n",
    "    country = None\n",
    "    country_code = None\n",
    "    for c in comp:\n",
    "        if \"country\" in c.get(\"types\", []):\n",
    "            country = c.get(\"long_name\")\n",
    "            country_code = c.get(\"short_name\")\n",
    "            break\n",
    "    return country, country_code\n",
    "\n",
    "def classify_locations(locations):\n",
    "    cache = load_cache(cache_file)\n",
    "    out = []\n",
    "    for loc in locations:\n",
    "        loc_norm = normalize(loc)\n",
    "        if loc_norm in cache:\n",
    "            geores = cache[loc_norm]\n",
    "        else:\n",
    "            try:\n",
    "                j = google_geocode(loc_norm, api_key)\n",
    "            except Exception as e:\n",
    "                print(\"Request error for:\", loc_norm, e)\n",
    "                j = None\n",
    "            cache[loc_norm] = j\n",
    "            save_cache(cache_file, cache)\n",
    "            time.sleep(rate_sleep)\n",
    "        country, country_code = get_country_from_result(cache[loc_norm])\n",
    "        out.append({\n",
    "            \"raw\": loc,\n",
    "            \"normalized\": loc_norm,\n",
    "            \"country\": country,\n",
    "            \"country_code\": country_code,\n",
    "            \"geocode_json\": cache[loc_norm]\n",
    "        })\n",
    "    return cache, pd.DataFrame(out)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    locs = load_docx_lines(input_docx)\n",
    "    cache, df = classify_locations(locs)\n",
    "\n",
    "    df['is_india'] = df['country_code'].fillna('').str.upper() == 'IN'\n",
    "    \n",
    "    # fallback if address contains 'India'\n",
    "    df.loc[~df['is_india'], 'is_india'] = df.loc[~df['is_india'], 'geocode_json'].apply(\n",
    "        lambda j: bool(j and any('India' in r.get('formatted_address','') for r in (j.get('results') if j else [])))\n",
    "    )\n",
    "\n",
    "    domestic_df = df[df['is_india']].copy()\n",
    "    foreign_df = df[~df['is_india']].copy()\n",
    "    unknown_df = df[df['country_code'].isnull() & df['geocode_json'].notnull()].copy()  # geocoded but no country\n",
    "\n",
    "    domestic_df.to_csv(RESULTS_DIR/\"domestic_df.csv\", index=False)\n",
    "    foreign_df.to_csv(RESULTS_DIR/\"foreign_df.csv\", index=False)\n",
    "    unknown_df.to_csv(RESULTS_DIR/\"unknown_df.csv\", index=False)\n",
    "\n",
    "    print(\"Results in\", RESULTS_DIR.absolute())\n",
    "    print(\"Domestic:\", len(domestic_df), \"Foreign:\", len(foreign_df), \"Unknown/NA:\", len(ambiguous_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4910b88-632d-4c33-ac34-6726a237e4b2",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Put key into api_key. Geocoding API may need to be enabled in Google Cloud with billing.\n",
    "2. Use cache — geocode_cache.json — to avoid re-querying same names if re-running the script.\n",
    "3. If quota is hit or billing limits, reduce rate_sleep or perform in small batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b1241-22ac-4e9a-b75c-163590833cf1",
   "metadata": {},
   "source": [
    "## Nominatim (OpenStreetMap) via geopy\n",
    "(No API key, polite usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c0050-748d-4cff-97a8-5f6454d63731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "## Variable names are capitalized to prevent overwriting variables written in the previous cell for Google Maps API call\n",
    "INPUT_DOCX = \"/mnt/data/Todcheck.docx\" \n",
    "CACHE_FILE = \"geocode_cache_nominatim.json\"\n",
    "RESULTS_DIR = Path(\"results_nominatim\"); RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def load_docx_lines(path):\n",
    "    from docx import Document\n",
    "    doc = Document(path)\n",
    "    lines = []\n",
    "    for p in doc.paragraphs:\n",
    "        txt = p.text.strip()\n",
    "        if txt:\n",
    "            txt = re.sub(r'^\\d+\\.\\s*', '', txt)\n",
    "            lines.append(txt)\n",
    "    return lines\n",
    "\n",
    "def normalize(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = re.sub(r'\\s+', ' ', str(s).strip())\n",
    "    return s\n",
    "\n",
    "def load_cache(fn):\n",
    "    if Path(fn).exists():\n",
    "        return json.loads(Path(fn).read_text(encoding=\"utf8\"))\n",
    "    return {}\n",
    "\n",
    "def save_cache(fn, data):\n",
    "    Path(fn).write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf8\")\n",
    "\n",
    "def classify_locations_nominatim(locs):\n",
    "    geolocator = Nominatim(user_agent=\"loc-classifier-1.0\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=2, error_wait_seconds=5)\n",
    "    cache = load_cache(CACHE_FILE)\n",
    "    rows = []\n",
    "    for loc in locs:\n",
    "        loc_norm = normalize(loc)\n",
    "        if not loc_norm:\n",
    "            rows.append({\"raw\":loc, \"normalized\":loc_norm, \"country\":None, \"country_code\":None, \"raw_geocode\":None})\n",
    "            continue\n",
    "        if loc_norm in cache:\n",
    "            res = cache[loc_norm]\n",
    "        else:\n",
    "            try:\n",
    "                place = geocode(loc_norm, addressdetails=True, exactly_one=True, timeout=20)\n",
    "            except Exception as e:\n",
    "                print(\"Error geocoding\", loc_norm, e)\n",
    "                place = None\n",
    "            if place:\n",
    "                cache[loc_norm] = {\"raw\": place.raw}\n",
    "            else:\n",
    "                cache[loc_norm] = None\n",
    "            save_cache(CACHE_FILE, cache)\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "            res = cache[loc_norm]\n",
    "        country = None\n",
    "        country_code = None\n",
    "        if res and 'raw' in res:\n",
    "            addr = res['raw'].get('address', {})\n",
    "            country = addr.get('country')\n",
    "            country_code = addr.get('country_code', '').upper() if addr.get('country_code') else None\n",
    "        rows.append({\"raw\": loc, \"normalized\": loc_norm, \"country\": country, \"country_code\": country_code, \"raw_geocode\": res})\n",
    "    return pd.DataFrame(rows), cache\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    locs = load_docx_lines(INPUT_DOCX)\n",
    "    df2, cache = classify_locations_nominatim(locs)\n",
    "    \n",
    "    df2['is_india'] = df2['country_code'] == 'IN'\n",
    "    df2.loc[~df['is_india'], 'is_india'] = df2.loc[~df2['is_india'], 'country'].fillna('').str.contains('India', case=False)\n",
    "    \n",
    "    domestic = df2[df2['is_india']].copy()\n",
    "    foreign = df2[~df2['is_india']].copy()\n",
    "    unknown = df2[df2['country'].isnull()].copy() # geocoded but no country\n",
    "    \n",
    "    domestic.to_csv(RESULTS_DIR/\"domestic_df2.csv\", index=False)\n",
    "    foreign.to_csv(RESULTS_DIR/\"foreign_df2.csv\", index=False)\n",
    "    unknown.to_csv(RESULTS_DIR/\"unknown.csv\", index=False)\n",
    "    print(\"Domestic:\", len(domestic), \"Foreign:\", len(foreign), \"Unknown:\", len(ambiguous))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ee1f7-8682-447d-9b25-75cea58ddccd",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. At most 1 request per second for bulk. Use RateLimiter min_delay_seconds=1.\n",
    "2. For over 1000+ unique rows for locations, this will take 1+ hours due to rate limit. Use caching to resume.\n",
    "3. Accuracy may vary for ambiguous short names (e.g., \"Springfield\" — many countries have that) -> These cases require manual checks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
